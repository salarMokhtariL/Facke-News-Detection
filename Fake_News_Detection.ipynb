{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNarNf7FnKkRg5w29r5jM/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salarMokhtariL/Facke-News-Detection/blob/main/Fake_News_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detection\n",
        "> By Salar Mokhtari Laleh"
      ],
      "metadata": {
        "id": "ndc7_491r82o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Libraries\n",
        "\n",
        "Importing the necessary libraries:\n"
      ],
      "metadata": {
        "id": "fxiA1oNssM2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hs1IMiUVsZzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "from transformers import DistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "D9GJ-oofr8ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset\n",
        "\n",
        "Load the dataset into a pandas DataFrame\n"
      ],
      "metadata": {
        "id": "GcBVA15rscR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/salarMokhtariL/Facke-News-Detection/main/Dataset/train.csv')\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/salarMokhtariL/Facke-News-Detection/main/Dataset/test.csv')"
      ],
      "metadata": {
        "id": "zxuSojQnsYLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "c0-eeQKO6HIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Data\n",
        "Prepare the data for the PyTorch model. First, let's define a custom dataset class"
      ],
      "metadata": {
        "id": "t9h859kos6k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' This class takes in the data, tokenizes it using the DistilBertTokenizer from the transformers library,\n",
        " and returns the input IDs, attention masks, and labels.'''\n",
        "\n",
        " \n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, data, max_len=128):\n",
        "        self.data = data\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.iloc[index]['text']\n",
        "        label = self.data.iloc[index]['label']\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0), torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "9-4ULm_Ps1f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into training and validation sets\n",
        "\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, \n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "D3t0jbkdtV02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PyTorch data loaders for the training, validation, and test sets:\n",
        "\n",
        "train_dataset = FakeNewsDataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = FakeNewsDataset(val_data)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_dataset = FakeNewsDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "qoRnpQ_4tYr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Model\n",
        "define the PyTorch model. We'll use the `DistilBertForSequenceClassification` model from the `transformers` library:"
      ],
      "metadata": {
        "id": "uXoyz5NiuR7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FakeNewsClassifier(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(FakeNewsClassifier, self).__init__()\n",
        "        self.bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs[0]"
      ],
      "metadata": {
        "id": "e5T1YcxauKpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n",
        "\n",
        "With the data and model prepared, we can now train the model using PyTorch. We'll define a function to train the model for one epoch"
      ],
      "metadata": {
        "id": "T5opyKlLujgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, optimizer, criterion, train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    for input_ids, attention_mask, labels in tqdm(train_loader, desc='Training'):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (outputs.argmax(1) == labels.to(device)).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc /= len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "MJT_-swAujJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes in the model, optimizer, loss function, and data loader, and performs a forward pass through the model, calculates the loss, and performs backpropagation and gradient descent to update the model parameters"
      ],
      "metadata": {
        "id": "YRTL30JYusZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also define a function to evaluate the model on the validation set:"
      ],
      "metadata": {
        "id": "uXAZkgv8utqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_epoch(model, criterion, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in tqdm(val_loader, desc='Validation'):\n",
        "            outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += (outputs.argmax(1) == labels.to(device)).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc /= len(val_loader.dataset)\n",
        "\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "Fz5hGObLucd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes in the model, loss function, and data loader, and performs a forward pass through the model to calculate the loss and accuracy on the validation set."
      ],
      "metadata": {
        "id": "tfYQ5ey_uwhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define the main training loop:"
      ],
      "metadata": {
        "id": "Gaq98MZWuyDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = FakeNewsClassifier().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader)\n",
        "    val_loss, val_acc = eval_epoch(model, criterion, val_loader)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        best_val_acc = val_acc"
      ],
      "metadata": {
        "id": "ZQCu1YIQuvTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l49LSMPT2_Qq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}